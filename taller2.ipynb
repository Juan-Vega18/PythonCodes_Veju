{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUCIÓN TALLER #2\n",
    "\n",
    "1. Considere una red neural con 2 variables de entrada y una sola capa oculta con 2 neuronas, que se conectan a una única neurona de salida, es decir 5 neuronas en total. Asuma que para todas las neuronas se utiliza el sigmoide como función de activación. ¿Cuántos parámetros entrenables o pesos tendría este modelo sencillo? Como función objetivo o de pérdidas, se utiliza el error cuadrático promedio (Mean squared error), asuma que el bias de todas las neuronas es cero. Describa cómo podría implementarse el algoritmo de backpropagation para entrenar esta red. Encuentre expresiones analíticas para el gradiente de la función de pérdidas con respecto a todos los parámetros entrenables del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución: \n",
    "\n",
    "1. En ese caso tenemos 2 variables de entrada, 2 neuronas en la capa oculta, 1 neurona de salida, en total 5 neuronas. \n",
    "Para las conexiones entre la capa de entrada y la capa oculta habrán 2 pesos por capa, es decir 4 pesos en total, además por cada neuoran en la capa oculta habrá un sesgo bias, serán 2 pero en \n",
    "este caso serán 0. \n",
    "\n",
    "Para las conexiones entre la capa oculta y la capa de salida habrán dos neuronas * 1 peso por cada una = 2 pesos en total. La neurona de salida tendrá un sesgo también. \n",
    "\n",
    "Entonces, el número total de parámetros entrenables será: \n",
    "\n",
    "a. Pesos entre la capa de entrada y la capa oculta: 4\n",
    "b. Pesos entre la capa oculta y la capa de salida: 2\n",
    "\n",
    "Entonces tenemos 6 parámetros entrenables teniendo en cuenta que los sesgos biasson 0. \n",
    "\n",
    "2. Como podría implementarse el algoritmo de backpropagation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1er paso, definimos la función sigmoide \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Gradiente de la función de pérdida con respecto a v_j\n",
    "def grad_loss_wrt_output_weights(y, y_target, h_j):\n",
    "    return (y - y_target) * y * (1 - y) * h_j\n",
    "\n",
    "# Gradiente de la función de pérdida con respecto a w_ij\n",
    "def grad_loss_wrt_hidden_weights(y, y_target, v_j, h_j, x_i):\n",
    "    return (y - y_target) * y * (1 - y) * v_j * h_j * (1 - h_j) * x_i\n",
    "\n",
    "# Voy a tomar unos datos de ejemplo para una prueba \n",
    "def main():\n",
    "    # Datos de entrada\n",
    "    x1 = 0.5\n",
    "    x2 = 0.3\n",
    "    \n",
    "    # Pesos\n",
    "    w11 = 0.4\n",
    "    w12 = -0.2\n",
    "    w21 = 0.1\n",
    "    w22 = 0.8\n",
    "    v1 = -0.5\n",
    "    v2 = 0.7\n",
    "    \n",
    "    # Calculamos las salidas de la capa oculta\n",
    "    h1 = sigmoid(w11 * x1 + w21 * x2)\n",
    "    h2 = sigmoid(w12 * x1 + w22 * x2)\n",
    "    \n",
    "    # Calculamos la salida de la capa de salida\n",
    "    y = sigmoid(v1 * h1 + v2 * h2)\n",
    "    \n",
    "    # Target\n",
    "    y_target = 0.8\n",
    "    \n",
    "    # Gradientes\n",
    "    grad_v1 = grad_loss_wrt_output_weights(y, y_target, h1)\n",
    "    grad_v2 = grad_loss_wrt_output_weights(y, y_target, h2)\n",
    "    grad_w11 = grad_loss_wrt_hidden_weights(y, y_target, v1, h1, x1)\n",
    "    grad_w12 = grad_loss_wrt_hidden_weights(y, y_target, v2, h2, x1)\n",
    "    grad_w21 = grad_loss_wrt_hidden_weights(y, y_target, v1, h1, x2)\n",
    "    grad_w22 = grad_loss_wrt_hidden_weights(y, y_target, v2, h2, x2)\n",
    "    \n",
    "    print(\"Gradiente de pérdida con respecto a v1:\", grad_v1)\n",
    "    print(\"Gradiente de pérdida con respecto a v2:\", grad_v2)\n",
    "    print(\"Gradiente de pérdida con respecto a w11:\", grad_w11)\n",
    "    print(\"Gradiente de pérdida con respecto a w12:\", grad_w12)\n",
    "    print(\"Gradiente de pérdida con respecto a w21:\", grad_w21)\n",
    "    print(\"Gradiente de pérdida con respecto a w22:\", grad_w22)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punto 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install torch pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4034, 21)\n"
     ]
    }
   ],
   "source": [
    "#Leer datos csv\n",
    "\n",
    "data = pd.read_csv(\"Datos GHF\\global_ghf.csv\")\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener valores x e y para la predicción\n",
    "\n",
    "x = data.iloc[:, np.r_[0:14, 16:21]].values  # Selecciona todas las columnas excepto la columna 15 (que es donde está el valor de GHF)\n",
    "y = data.iloc[:, 15].values #Seleccionamos la columna 15 exacta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en datos de entrenamiento y datos de prueba \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamos los datos\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos los datos a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un DataLoader para facilitar el manejo de los datos durante el entrenamiento, basicamente los vamos a cargar \n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) #El bach lo podemos ir cambiando según como veamos el rendimiento del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una clase que será la arquitectura de la red neuronal\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)  # Capa completamente conectada\n",
    "        self.fc2 = nn.Linear(32, 16)          # Capa completamente conectada\n",
    "        self.fc3 = nn.Linear(16, 1)           # Capa de salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Función de activación ReLU en la primera capa oculta, en este caso vamos a usar ReLU pero podemos ubicar otras funciones\n",
    "                                     # de activación si obtenemos resultados no tan favorables\n",
    "        x = torch.relu(self.fc2(x))  # Función de activación ReLU en la segunda capa oculta\n",
    "        x = self.fc3(x)              # Sin función de activación en la capa de salida\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar el modelo\n",
    "input_size = X_train.shape[1]\n",
    "model = NeuralNetwork(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de pérdida y el optimizador\n",
    "criterion = nn.MSELoss()                   # Mean Squared Error Loss, es la que usamos en el taller 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimizador Adam (esto es generalmente usado, lo tomé de ejemplo pero hay muchos optimizadores, no he probado muchos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.6621\n",
      "Epoch [2/10], Loss: 0.4253\n",
      "Epoch [3/10], Loss: 0.1682\n",
      "Epoch [4/10], Loss: 0.1609\n",
      "Epoch [5/10], Loss: 0.1885\n",
      "Epoch [6/10], Loss: 0.1265\n",
      "Epoch [7/10], Loss: 0.1759\n",
      "Epoch [8/10], Loss: 0.0771\n",
      "Epoch [9/10], Loss: 0.0883\n",
      "Epoch [10/10], Loss: 0.1058\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "num_epochs = 10 #Lo vamos modificando en función de la calidad de los resultados\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()            # Reiniciar los gradientes acumulados (es importante para las iteraciones)\n",
    "        outputs = model(inputs)         # Forward pass\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))  # Calcular la pérdida (iremos ajustando las pérdidas para ir mejorando el modelo)\n",
    "        loss.backward()                 # Backward pass\n",
    "        optimizer.step()                # Actualizar los parámetros\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0901\n"
     ]
    }
   ],
   "source": [
    "#Probamos el modelo para ver su calidad \n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    test_loss = criterion(y_pred, y_test_tensor.unsqueeze(1))\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "#En esta prueba, obtuvimos una pérdida en nuestro test de 0.06, bastante buena, básicamente podemos decir que el modelo tiene un excelente nivel de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo \n",
    "\n",
    "torch.save(model.state_dict(), \"modelo_entrenado.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=19, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(input_size)  \n",
    "model.load_state_dict(torch.load(\"modelo_entrenado.pth\"))\n",
    "model.eval()  # Modelo en modo de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos unos datos de ejemplo\n",
    "data_test = [0,12.999,13,0.060800001,2048.46,2135.04,3879.25,1764.58,30.50409809,1.980000019,36.3899994,622.95,-110.333313,75.50012207,2.29706001,-1.69836,-42.0954018,219571,110.1439972,76.2264023]\n",
    "# El modelo debe predecir GHF de 54\n",
    "\n",
    "data_tensor = torch.tensor(data_test, dtype=torch.float32)\n",
    "\n",
    "new_data_tensor = data_tensor.view(20, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(new_data_tensor)\n",
    "\n",
    "print(\"Predicciones:\")\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
